{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Models -- A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.\n",
    "- Author: Sebastian Raschka\n",
    "- GitHub Repository: https://github.com/rasbt/deeplearning-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "\n",
      "CPython 3.7.3\n",
      "IPython 7.9.0\n",
      "\n",
      "torch 1.7.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Runs on CPU or GPU (if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo -- Reproducible Results with Deterministic Behavior and Runtime Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are benchmarking the performance impact of setting PyTorch to deterministic behavior. In general, there are two aspects for reproducible resuls in PyTorch, \n",
    "1. Setting a random seed\n",
    "2. Setting cuDNN and PyTorch algorithmic behavior to deterministic\n",
    "\n",
    "For more details, please see https://pytorch.org/docs/stable/notes/randomness.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting a random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend using a function like the following one prior to using dataset loaders and initializing a model if you want to ensure the data is shuffled in the same manner if you rerun this notebook and the model gets the same initial random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting cuDNN and PyTorch algorithmic behavior to deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the `set_all_seeds` function above, I recommend setting the behavior of PyTorch and cuDNN to deterministic (this is particulary relevant when using GPUs). We can also define a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_deterministic():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    torch.set_deterministic(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the general configuration in this section, the following two sections will train a ResNet-101 model without and with deterministic behavior to get a sense how using deterministic options affect the runtime speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# Data settings\n",
    "num_classes = 10\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\") # to include ../helper_evaluate.py etc.\n",
    "\n",
    "from helper_evaluate import compute_accuracy\n",
    "from helper_data import get_dataloaders_cifar10\n",
    "from helper_train import train_classifier_simple_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Run without Deterministic Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we enable deterministic behavior, we will run a ResNet-101 with otherwise the exact same settings for comparison. Note that setting random seeds doesn't affect the timing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set random seed ###\n",
    "set_all_seeds(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### Dataset\n",
    "##########################\n",
    "\n",
    "train_loader, valid_loader, test_loader = get_dataloaders_cifar10(\n",
    "    batch_size, \n",
    "    num_workers=0, \n",
    "    validation_fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### Model\n",
    "##########################\n",
    "\n",
    "\n",
    "from deterministic_benchmark_utils import resnet101\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = resnet101(num_classes, grayscale=False)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Batch 0000/0352 | Loss: 2.6711\n",
      "Epoch: 001/050 | Batch 0200/0352 | Loss: 2.9076\n",
      "Epoch: 001/050 | Train Acc.: 10.373% |  Loss: 3.180\n",
      "Epoch: 001/050 | Validation Acc.: 10.120% |  Loss: 3.161\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 002/050 | Batch 0000/0352 | Loss: 2.1174\n",
      "Epoch: 002/050 | Batch 0200/0352 | Loss: 1.9204\n",
      "Epoch: 002/050 | Train Acc.: 26.449% |  Loss: 1.886\n",
      "Epoch: 002/050 | Validation Acc.: 26.340% |  Loss: 1.872\n",
      "Time elapsed: 2.47 min\n",
      "Epoch: 003/050 | Batch 0000/0352 | Loss: 1.7529\n",
      "Epoch: 003/050 | Batch 0200/0352 | Loss: 1.7582\n",
      "Epoch: 003/050 | Train Acc.: 35.082% |  Loss: 1.731\n",
      "Epoch: 003/050 | Validation Acc.: 34.200% |  Loss: 1.733\n",
      "Time elapsed: 3.72 min\n",
      "Epoch: 004/050 | Batch 0000/0352 | Loss: 1.6620\n",
      "Epoch: 004/050 | Batch 0200/0352 | Loss: 1.6138\n",
      "Epoch: 004/050 | Train Acc.: 44.818% |  Loss: 1.538\n",
      "Epoch: 004/050 | Validation Acc.: 43.380% |  Loss: 1.562\n",
      "Time elapsed: 4.98 min\n",
      "Epoch: 005/050 | Batch 0000/0352 | Loss: 1.4111\n",
      "Epoch: 005/050 | Batch 0200/0352 | Loss: 1.4117\n",
      "Epoch: 005/050 | Train Acc.: 48.462% |  Loss: 1.552\n",
      "Epoch: 005/050 | Validation Acc.: 47.480% |  Loss: 1.592\n",
      "Time elapsed: 6.23 min\n",
      "Epoch: 006/050 | Batch 0000/0352 | Loss: 1.2027\n",
      "Epoch: 006/050 | Batch 0200/0352 | Loss: 1.3274\n",
      "Epoch: 006/050 | Train Acc.: 45.511% |  Loss: 1.657\n",
      "Epoch: 006/050 | Validation Acc.: 43.840% |  Loss: 1.727\n",
      "Time elapsed: 7.49 min\n",
      "Epoch: 007/050 | Batch 0000/0352 | Loss: 1.1930\n",
      "Epoch: 007/050 | Batch 0200/0352 | Loss: 1.0650\n",
      "Epoch: 007/050 | Train Acc.: 57.940% |  Loss: 1.216\n",
      "Epoch: 007/050 | Validation Acc.: 55.020% |  Loss: 1.331\n",
      "Time elapsed: 8.74 min\n",
      "Epoch: 008/050 | Batch 0000/0352 | Loss: 0.8951\n",
      "Epoch: 008/050 | Batch 0200/0352 | Loss: 1.0024\n",
      "Epoch: 008/050 | Train Acc.: 66.362% |  Loss: 0.974\n",
      "Epoch: 008/050 | Validation Acc.: 63.700% |  Loss: 1.065\n",
      "Time elapsed: 9.99 min\n",
      "Epoch: 009/050 | Batch 0000/0352 | Loss: 0.8176\n",
      "Epoch: 009/050 | Batch 0200/0352 | Loss: 0.9750\n",
      "Epoch: 009/050 | Train Acc.: 69.022% |  Loss: 0.903\n",
      "Epoch: 009/050 | Validation Acc.: 65.160% |  Loss: 1.048\n",
      "Time elapsed: 11.24 min\n",
      "Epoch: 010/050 | Batch 0000/0352 | Loss: 0.6964\n",
      "Epoch: 010/050 | Batch 0200/0352 | Loss: 0.8306\n",
      "Epoch: 010/050 | Train Acc.: 68.058% |  Loss: 0.933\n",
      "Epoch: 010/050 | Validation Acc.: 63.920% |  Loss: 1.059\n",
      "Time elapsed: 12.49 min\n",
      "Epoch: 011/050 | Batch 0000/0352 | Loss: 0.6287\n",
      "Epoch: 011/050 | Batch 0200/0352 | Loss: 0.7196\n",
      "Epoch: 011/050 | Train Acc.: 64.816% |  Loss: 1.021\n",
      "Epoch: 011/050 | Validation Acc.: 60.300% |  Loss: 1.138\n",
      "Time elapsed: 13.75 min\n",
      "Epoch: 012/050 | Batch 0000/0352 | Loss: 0.9567\n",
      "Epoch: 012/050 | Batch 0200/0352 | Loss: 0.7519\n",
      "Epoch: 012/050 | Train Acc.: 78.964% |  Loss: 0.610\n",
      "Epoch: 012/050 | Validation Acc.: 72.320% |  Loss: 0.800\n",
      "Time elapsed: 15.00 min\n",
      "Epoch: 013/050 | Batch 0000/0352 | Loss: 0.7314\n",
      "Epoch: 013/050 | Batch 0200/0352 | Loss: 0.7673\n",
      "Epoch: 013/050 | Train Acc.: 77.604% |  Loss: 0.646\n",
      "Epoch: 013/050 | Validation Acc.: 70.760% |  Loss: 0.878\n",
      "Time elapsed: 16.25 min\n",
      "Epoch: 014/050 | Batch 0000/0352 | Loss: 0.4963\n",
      "Epoch: 014/050 | Batch 0200/0352 | Loss: 0.4835\n",
      "Epoch: 014/050 | Train Acc.: 77.976% |  Loss: 0.665\n",
      "Epoch: 014/050 | Validation Acc.: 68.660% |  Loss: 0.974\n",
      "Time elapsed: 17.50 min\n",
      "Epoch: 015/050 | Batch 0000/0352 | Loss: 0.4561\n",
      "Epoch: 015/050 | Batch 0200/0352 | Loss: 0.4469\n",
      "Epoch: 015/050 | Train Acc.: 53.378% |  Loss: 1.501\n",
      "Epoch: 015/050 | Validation Acc.: 51.220% |  Loss: 1.570\n",
      "Time elapsed: 18.75 min\n",
      "Epoch: 016/050 | Batch 0000/0352 | Loss: 1.2047\n",
      "Epoch: 016/050 | Batch 0200/0352 | Loss: 1.0445\n",
      "Epoch: 016/050 | Train Acc.: 84.716% |  Loss: 0.458\n",
      "Epoch: 016/050 | Validation Acc.: 75.300% |  Loss: 0.727\n",
      "Time elapsed: 20.00 min\n",
      "Epoch: 017/050 | Batch 0000/0352 | Loss: 0.3897\n",
      "Epoch: 017/050 | Batch 0200/0352 | Loss: 0.3842\n",
      "Epoch: 017/050 | Train Acc.: 84.636% |  Loss: 0.456\n",
      "Epoch: 017/050 | Validation Acc.: 73.300% |  Loss: 0.822\n",
      "Time elapsed: 21.25 min\n",
      "Epoch: 018/050 | Batch 0000/0352 | Loss: 0.2760\n",
      "Epoch: 018/050 | Batch 0200/0352 | Loss: 0.4514\n",
      "Epoch: 018/050 | Train Acc.: 90.289% |  Loss: 0.296\n",
      "Epoch: 018/050 | Validation Acc.: 75.200% |  Loss: 0.801\n",
      "Time elapsed: 22.50 min\n",
      "Epoch: 019/050 | Batch 0000/0352 | Loss: 0.2528\n",
      "Epoch: 019/050 | Batch 0200/0352 | Loss: 0.3562\n",
      "Epoch: 019/050 | Train Acc.: 90.789% |  Loss: 0.275\n",
      "Epoch: 019/050 | Validation Acc.: 74.740% |  Loss: 0.855\n",
      "Time elapsed: 23.74 min\n",
      "Epoch: 020/050 | Batch 0000/0352 | Loss: 0.1820\n",
      "Epoch: 020/050 | Batch 0200/0352 | Loss: 0.3345\n",
      "Epoch: 020/050 | Train Acc.: 90.000% |  Loss: 0.291\n",
      "Epoch: 020/050 | Validation Acc.: 73.660% |  Loss: 0.951\n",
      "Time elapsed: 24.99 min\n",
      "Epoch: 021/050 | Batch 0000/0352 | Loss: 0.1543\n",
      "Epoch: 021/050 | Batch 0200/0352 | Loss: 0.3990\n",
      "Epoch: 021/050 | Train Acc.: 79.656% |  Loss: 0.586\n",
      "Epoch: 021/050 | Validation Acc.: 67.860% |  Loss: 0.987\n",
      "Time elapsed: 26.25 min\n",
      "Epoch: 022/050 | Batch 0000/0352 | Loss: 0.4400\n",
      "Epoch: 022/050 | Batch 0200/0352 | Loss: 0.3163\n",
      "Epoch: 022/050 | Train Acc.: 69.487% |  Loss: 1.070\n",
      "Epoch: 022/050 | Validation Acc.: 59.960% |  Loss: 1.528\n",
      "Time elapsed: 27.50 min\n",
      "Epoch: 023/050 | Batch 0000/0352 | Loss: 0.4916\n",
      "Epoch: 023/050 | Batch 0200/0352 | Loss: 0.4553\n",
      "Epoch: 023/050 | Train Acc.: 92.216% |  Loss: 0.238\n",
      "Epoch: 023/050 | Validation Acc.: 74.580% |  Loss: 0.847\n",
      "Time elapsed: 28.75 min\n",
      "Epoch: 024/050 | Batch 0000/0352 | Loss: 0.1891\n",
      "Epoch: 024/050 | Batch 0200/0352 | Loss: 0.2221\n",
      "Epoch: 024/050 | Train Acc.: 95.013% |  Loss: 0.149\n",
      "Epoch: 024/050 | Validation Acc.: 75.960% |  Loss: 0.947\n",
      "Time elapsed: 30.00 min\n",
      "Epoch: 025/050 | Batch 0000/0352 | Loss: 0.1014\n",
      "Epoch: 025/050 | Batch 0200/0352 | Loss: 0.1063\n",
      "Epoch: 025/050 | Train Acc.: 92.400% |  Loss: 0.229\n",
      "Epoch: 025/050 | Validation Acc.: 72.100% |  Loss: 1.156\n",
      "Time elapsed: 31.23 min\n",
      "Epoch: 026/050 | Batch 0000/0352 | Loss: 0.0897\n",
      "Epoch: 026/050 | Batch 0200/0352 | Loss: 0.1512\n",
      "Epoch: 026/050 | Train Acc.: 90.338% |  Loss: 0.315\n",
      "Epoch: 026/050 | Validation Acc.: 70.880% |  Loss: 1.295\n",
      "Time elapsed: 32.48 min\n",
      "Epoch: 027/050 | Batch 0000/0352 | Loss: 0.0792\n",
      "Epoch: 027/050 | Batch 0200/0352 | Loss: 0.2257\n",
      "Epoch: 027/050 | Train Acc.: 80.938% |  Loss: 0.719\n",
      "Epoch: 027/050 | Validation Acc.: 64.880% |  Loss: 1.651\n",
      "Time elapsed: 33.72 min\n",
      "Epoch: 028/050 | Batch 0000/0352 | Loss: 0.1038\n",
      "Epoch: 028/050 | Batch 0200/0352 | Loss: 0.0842\n",
      "Epoch: 028/050 | Train Acc.: 96.373% |  Loss: 0.109\n",
      "Epoch: 028/050 | Validation Acc.: 75.580% |  Loss: 1.016\n",
      "Time elapsed: 34.97 min\n",
      "Epoch: 029/050 | Batch 0000/0352 | Loss: 0.1045\n",
      "Epoch: 029/050 | Batch 0200/0352 | Loss: 0.0580\n",
      "Epoch: 029/050 | Train Acc.: 80.776% |  Loss: 0.660\n",
      "Epoch: 029/050 | Validation Acc.: 64.660% |  Loss: 1.545\n",
      "Time elapsed: 36.21 min\n",
      "Epoch: 030/050 | Batch 0000/0352 | Loss: 0.0828\n",
      "Epoch: 030/050 | Batch 0200/0352 | Loss: 0.0998\n",
      "Epoch: 030/050 | Train Acc.: 94.238% |  Loss: 0.174\n",
      "Epoch: 030/050 | Validation Acc.: 74.460% |  Loss: 1.173\n",
      "Time elapsed: 37.46 min\n",
      "Epoch: 031/050 | Batch 0000/0352 | Loss: 0.0551\n",
      "Epoch: 031/050 | Batch 0200/0352 | Loss: 0.0713\n",
      "Epoch: 031/050 | Train Acc.: 96.702% |  Loss: 0.098\n",
      "Epoch: 031/050 | Validation Acc.: 75.640% |  Loss: 1.070\n",
      "Time elapsed: 38.70 min\n",
      "Epoch: 032/050 | Batch 0000/0352 | Loss: 0.0750\n",
      "Epoch: 032/050 | Batch 0200/0352 | Loss: 0.1183\n",
      "Epoch: 032/050 | Train Acc.: 95.442% |  Loss: 0.136\n",
      "Epoch: 032/050 | Validation Acc.: 74.840% |  Loss: 1.128\n",
      "Time elapsed: 39.94 min\n",
      "Epoch: 033/050 | Batch 0000/0352 | Loss: 0.0645\n",
      "Epoch: 033/050 | Batch 0200/0352 | Loss: 0.1217\n",
      "Epoch: 033/050 | Train Acc.: 93.558% |  Loss: 0.199\n",
      "Epoch: 033/050 | Validation Acc.: 72.700% |  Loss: 1.221\n",
      "Time elapsed: 41.18 min\n",
      "Epoch: 034/050 | Batch 0000/0352 | Loss: 0.0559\n",
      "Epoch: 034/050 | Batch 0200/0352 | Loss: 0.0312\n",
      "Epoch: 034/050 | Train Acc.: 95.860% |  Loss: 0.124\n",
      "Epoch: 034/050 | Validation Acc.: 74.780% |  Loss: 1.172\n",
      "Time elapsed: 42.41 min\n",
      "Epoch: 035/050 | Batch 0000/0352 | Loss: 0.0742\n",
      "Epoch: 035/050 | Batch 0200/0352 | Loss: 0.0606\n",
      "Epoch: 035/050 | Train Acc.: 95.847% |  Loss: 0.124\n",
      "Epoch: 035/050 | Validation Acc.: 74.760% |  Loss: 1.231\n",
      "Time elapsed: 43.65 min\n",
      "Epoch: 036/050 | Batch 0000/0352 | Loss: 0.0362\n",
      "Epoch: 036/050 | Batch 0200/0352 | Loss: 0.1028\n",
      "Epoch: 036/050 | Train Acc.: 91.842% |  Loss: 0.259\n",
      "Epoch: 036/050 | Validation Acc.: 71.360% |  Loss: 1.339\n",
      "Time elapsed: 44.89 min\n",
      "Epoch: 037/050 | Batch 0000/0352 | Loss: 0.0886\n",
      "Epoch: 037/050 | Batch 0200/0352 | Loss: 0.0395\n",
      "Epoch: 037/050 | Train Acc.: 96.227% |  Loss: 0.113\n",
      "Epoch: 037/050 | Validation Acc.: 75.000% |  Loss: 1.161\n",
      "Time elapsed: 46.13 min\n",
      "Epoch: 038/050 | Batch 0000/0352 | Loss: 0.0518\n",
      "Epoch: 038/050 | Batch 0200/0352 | Loss: 0.1040\n",
      "Epoch: 038/050 | Train Acc.: 96.878% |  Loss: 0.091\n",
      "Epoch: 038/050 | Validation Acc.: 75.740% |  Loss: 1.150\n",
      "Time elapsed: 47.36 min\n",
      "Epoch: 039/050 | Batch 0000/0352 | Loss: 0.0467\n",
      "Epoch: 039/050 | Batch 0200/0352 | Loss: 0.0517\n",
      "Epoch: 039/050 | Train Acc.: 97.733% |  Loss: 0.066\n",
      "Epoch: 039/050 | Validation Acc.: 75.640% |  Loss: 1.209\n",
      "Time elapsed: 48.60 min\n",
      "Epoch: 040/050 | Batch 0000/0352 | Loss: 0.0493\n",
      "Epoch: 040/050 | Batch 0200/0352 | Loss: 0.0755\n",
      "Epoch: 040/050 | Train Acc.: 96.107% |  Loss: 0.114\n",
      "Epoch: 040/050 | Validation Acc.: 74.400% |  Loss: 1.298\n",
      "Time elapsed: 49.84 min\n",
      "Epoch: 041/050 | Batch 0000/0352 | Loss: 0.0389\n",
      "Epoch: 041/050 | Batch 0200/0352 | Loss: 1.0367\n",
      "Epoch: 041/050 | Train Acc.: 71.216% |  Loss: 1.273\n",
      "Epoch: 041/050 | Validation Acc.: 64.340% |  Loss: 1.454\n",
      "Time elapsed: 51.08 min\n",
      "Epoch: 042/050 | Batch 0000/0352 | Loss: 0.7024\n",
      "Epoch: 042/050 | Batch 0200/0352 | Loss: 0.4050\n",
      "Epoch: 042/050 | Train Acc.: 95.367% |  Loss: 0.143\n",
      "Epoch: 042/050 | Validation Acc.: 75.920% |  Loss: 0.907\n",
      "Time elapsed: 52.32 min\n",
      "Epoch: 043/050 | Batch 0000/0352 | Loss: 0.1167\n",
      "Epoch: 043/050 | Batch 0200/0352 | Loss: 0.1382\n",
      "Epoch: 043/050 | Train Acc.: 97.558% |  Loss: 0.073\n",
      "Epoch: 043/050 | Validation Acc.: 75.780% |  Loss: 1.094\n",
      "Time elapsed: 53.56 min\n",
      "Epoch: 044/050 | Batch 0000/0352 | Loss: 0.0812\n",
      "Epoch: 044/050 | Batch 0200/0352 | Loss: 0.0096\n",
      "Epoch: 044/050 | Train Acc.: 99.002% |  Loss: 0.030\n",
      "Epoch: 044/050 | Validation Acc.: 76.100% |  Loss: 1.222\n",
      "Time elapsed: 54.80 min\n",
      "Epoch: 045/050 | Batch 0000/0352 | Loss: 0.0376\n",
      "Epoch: 045/050 | Batch 0200/0352 | Loss: 0.0210\n",
      "Epoch: 045/050 | Train Acc.: 96.371% |  Loss: 0.118\n",
      "Epoch: 045/050 | Validation Acc.: 74.380% |  Loss: 1.464\n",
      "Time elapsed: 56.05 min\n",
      "Epoch: 046/050 | Batch 0000/0352 | Loss: 0.1085\n",
      "Epoch: 046/050 | Batch 0200/0352 | Loss: 0.0573\n",
      "Epoch: 046/050 | Train Acc.: 93.153% |  Loss: 0.241\n",
      "Epoch: 046/050 | Validation Acc.: 72.460% |  Loss: 1.515\n",
      "Time elapsed: 57.29 min\n",
      "Epoch: 047/050 | Batch 0000/0352 | Loss: 0.0687\n",
      "Epoch: 047/050 | Batch 0200/0352 | Loss: 0.0383\n",
      "Epoch: 047/050 | Train Acc.: 97.544% |  Loss: 0.073\n",
      "Epoch: 047/050 | Validation Acc.: 75.480% |  Loss: 1.252\n",
      "Time elapsed: 58.53 min\n",
      "Epoch: 048/050 | Batch 0000/0352 | Loss: 0.0621\n",
      "Epoch: 048/050 | Batch 0200/0352 | Loss: 0.0192\n",
      "Epoch: 048/050 | Train Acc.: 97.813% |  Loss: 0.065\n",
      "Epoch: 048/050 | Validation Acc.: 76.180% |  Loss: 1.259\n",
      "Time elapsed: 59.77 min\n",
      "Epoch: 049/050 | Batch 0000/0352 | Loss: 0.0205\n",
      "Epoch: 049/050 | Batch 0200/0352 | Loss: 0.0514\n",
      "Epoch: 049/050 | Train Acc.: 94.984% |  Loss: 0.155\n",
      "Epoch: 049/050 | Validation Acc.: 74.220% |  Loss: 1.243\n",
      "Time elapsed: 61.02 min\n",
      "Epoch: 050/050 | Batch 0000/0352 | Loss: 0.0207\n",
      "Epoch: 050/050 | Batch 0200/0352 | Loss: 0.0189\n",
      "Epoch: 050/050 | Train Acc.: 92.847% |  Loss: 0.268\n",
      "Epoch: 050/050 | Validation Acc.: 72.580% |  Loss: 1.509\n",
      "Time elapsed: 62.26 min\n",
      "Total Training Time: 62.26 min\n"
     ]
    }
   ],
   "source": [
    "train_classifier_simple_v1(num_epochs=num_epochs, model=model, \n",
    "                           optimizer=optimizer, device=DEVICE, \n",
    "                           train_loader=train_loader, valid_loader=valid_loader, \n",
    "                           logging_interval=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Run with Deterministic Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we set the deterministic behavior via the `set_deterministic()` function defined at the top of this notebook and compare how it affects the runtime speed of the ResNet-101 model. (Note that setting random seeds doesn't affect the timing results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set random seed ###\n",
    "set_all_seeds(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### Dataset\n",
    "##########################\n",
    "\n",
    "train_loader, valid_loader, test_loader = get_dataloaders_cifar10(\n",
    "    batch_size, \n",
    "    num_workers=0, \n",
    "    validation_fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### Model\n",
    "##########################\n",
    "\n",
    "\n",
    "from deterministic_benchmark_utils import resnet101\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = resnet101(num_classes, grayscale=False)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Batch 0000/0352 | Loss: 2.6711\n",
      "Epoch: 001/050 | Batch 0200/0352 | Loss: 3.3140\n",
      "Epoch: 001/050 | Train Acc.: 24.200% |  Loss: 2.016\n",
      "Epoch: 001/050 | Validation Acc.: 23.920% |  Loss: 1.993\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 002/050 | Batch 0000/0352 | Loss: 1.8132\n",
      "Epoch: 002/050 | Batch 0200/0352 | Loss: 1.6629\n",
      "Epoch: 002/050 | Train Acc.: 36.464% |  Loss: 1.664\n",
      "Epoch: 002/050 | Validation Acc.: 35.900% |  Loss: 1.637\n",
      "Time elapsed: 2.49 min\n",
      "Epoch: 003/050 | Batch 0000/0352 | Loss: 1.5070\n",
      "Epoch: 003/050 | Batch 0200/0352 | Loss: 1.4028\n",
      "Epoch: 003/050 | Train Acc.: 44.527% |  Loss: 1.565\n",
      "Epoch: 003/050 | Validation Acc.: 43.400% |  Loss: 1.571\n",
      "Time elapsed: 3.74 min\n",
      "Epoch: 004/050 | Batch 0000/0352 | Loss: 1.3491\n",
      "Epoch: 004/050 | Batch 0200/0352 | Loss: 1.4037\n",
      "Epoch: 004/050 | Train Acc.: 51.798% |  Loss: 1.318\n",
      "Epoch: 004/050 | Validation Acc.: 50.020% |  Loss: 1.361\n",
      "Time elapsed: 4.98 min\n",
      "Epoch: 005/050 | Batch 0000/0352 | Loss: 1.2558\n",
      "Epoch: 005/050 | Batch 0200/0352 | Loss: 1.4114\n",
      "Epoch: 005/050 | Train Acc.: 59.113% |  Loss: 1.125\n",
      "Epoch: 005/050 | Validation Acc.: 57.980% |  Loss: 1.166\n",
      "Time elapsed: 6.23 min\n",
      "Epoch: 006/050 | Batch 0000/0352 | Loss: 1.0834\n",
      "Epoch: 006/050 | Batch 0200/0352 | Loss: 1.2596\n",
      "Epoch: 006/050 | Train Acc.: 57.069% |  Loss: 1.231\n",
      "Epoch: 006/050 | Validation Acc.: 56.800% |  Loss: 1.266\n",
      "Time elapsed: 7.48 min\n",
      "Epoch: 007/050 | Batch 0000/0352 | Loss: 1.0507\n",
      "Epoch: 007/050 | Batch 0200/0352 | Loss: 1.0661\n",
      "Epoch: 007/050 | Train Acc.: 63.062% |  Loss: 1.034\n",
      "Epoch: 007/050 | Validation Acc.: 61.500% |  Loss: 1.099\n",
      "Time elapsed: 8.73 min\n",
      "Epoch: 008/050 | Batch 0000/0352 | Loss: 0.9004\n",
      "Epoch: 008/050 | Batch 0200/0352 | Loss: 0.9308\n",
      "Epoch: 008/050 | Train Acc.: 66.742% |  Loss: 0.965\n",
      "Epoch: 008/050 | Validation Acc.: 64.880% |  Loss: 1.028\n",
      "Time elapsed: 9.97 min\n",
      "Epoch: 009/050 | Batch 0000/0352 | Loss: 0.8492\n",
      "Epoch: 009/050 | Batch 0200/0352 | Loss: 1.1439\n",
      "Epoch: 009/050 | Train Acc.: 65.771% |  Loss: 0.985\n",
      "Epoch: 009/050 | Validation Acc.: 63.860% |  Loss: 1.055\n",
      "Time elapsed: 11.22 min\n",
      "Epoch: 010/050 | Batch 0000/0352 | Loss: 0.9284\n",
      "Epoch: 010/050 | Batch 0200/0352 | Loss: 0.9031\n",
      "Epoch: 010/050 | Train Acc.: 71.144% |  Loss: 0.831\n",
      "Epoch: 010/050 | Validation Acc.: 66.740% |  Loss: 0.969\n",
      "Time elapsed: 12.47 min\n",
      "Epoch: 011/050 | Batch 0000/0352 | Loss: 0.5533\n",
      "Epoch: 011/050 | Batch 0200/0352 | Loss: 0.6613\n",
      "Epoch: 011/050 | Train Acc.: 58.164% |  Loss: 1.307\n",
      "Epoch: 011/050 | Validation Acc.: 55.380% |  Loss: 1.451\n",
      "Time elapsed: 13.72 min\n",
      "Epoch: 012/050 | Batch 0000/0352 | Loss: 0.6920\n",
      "Epoch: 012/050 | Batch 0200/0352 | Loss: 0.5449\n",
      "Epoch: 012/050 | Train Acc.: 80.302% |  Loss: 0.569\n",
      "Epoch: 012/050 | Validation Acc.: 72.540% |  Loss: 0.823\n",
      "Time elapsed: 14.98 min\n",
      "Epoch: 013/050 | Batch 0000/0352 | Loss: 0.6461\n",
      "Epoch: 013/050 | Batch 0200/0352 | Loss: 0.6549\n",
      "Epoch: 013/050 | Train Acc.: 80.560% |  Loss: 0.553\n",
      "Epoch: 013/050 | Validation Acc.: 72.340% |  Loss: 0.830\n",
      "Time elapsed: 16.24 min\n",
      "Epoch: 014/050 | Batch 0000/0352 | Loss: 0.4838\n",
      "Epoch: 014/050 | Batch 0200/0352 | Loss: 0.5160\n",
      "Epoch: 014/050 | Train Acc.: 85.240% |  Loss: 0.420\n",
      "Epoch: 014/050 | Validation Acc.: 74.100% |  Loss: 0.807\n",
      "Time elapsed: 17.49 min\n",
      "Epoch: 015/050 | Batch 0000/0352 | Loss: 0.3290\n",
      "Epoch: 015/050 | Batch 0200/0352 | Loss: 0.3866\n",
      "Epoch: 015/050 | Train Acc.: 86.860% |  Loss: 0.379\n",
      "Epoch: 015/050 | Validation Acc.: 74.280% |  Loss: 0.811\n",
      "Time elapsed: 18.74 min\n",
      "Epoch: 016/050 | Batch 0000/0352 | Loss: 0.2458\n",
      "Epoch: 016/050 | Batch 0200/0352 | Loss: 0.3639\n",
      "Epoch: 016/050 | Train Acc.: 68.911% |  Loss: 0.960\n",
      "Epoch: 016/050 | Validation Acc.: 60.980% |  Loss: 1.310\n",
      "Time elapsed: 19.99 min\n",
      "Epoch: 017/050 | Batch 0000/0352 | Loss: 0.4008\n",
      "Epoch: 017/050 | Batch 0200/0352 | Loss: 0.4119\n",
      "Epoch: 017/050 | Train Acc.: 88.831% |  Loss: 0.318\n",
      "Epoch: 017/050 | Validation Acc.: 74.860% |  Loss: 0.825\n",
      "Time elapsed: 21.24 min\n",
      "Epoch: 018/050 | Batch 0000/0352 | Loss: 0.2620\n",
      "Epoch: 018/050 | Batch 0200/0352 | Loss: 0.4014\n",
      "Epoch: 018/050 | Train Acc.: 88.718% |  Loss: 0.335\n",
      "Epoch: 018/050 | Validation Acc.: 72.340% |  Loss: 0.930\n",
      "Time elapsed: 22.49 min\n",
      "Epoch: 019/050 | Batch 0000/0352 | Loss: 0.1993\n",
      "Epoch: 019/050 | Batch 0200/0352 | Loss: 0.3162\n",
      "Epoch: 019/050 | Train Acc.: 87.980% |  Loss: 0.351\n",
      "Epoch: 019/050 | Validation Acc.: 72.580% |  Loss: 0.981\n",
      "Time elapsed: 23.74 min\n",
      "Epoch: 020/050 | Batch 0000/0352 | Loss: 0.1150\n",
      "Epoch: 020/050 | Batch 0200/0352 | Loss: 0.3635\n",
      "Epoch: 020/050 | Train Acc.: 88.631% |  Loss: 0.343\n",
      "Epoch: 020/050 | Validation Acc.: 73.100% |  Loss: 1.041\n",
      "Time elapsed: 25.01 min\n",
      "Epoch: 021/050 | Batch 0000/0352 | Loss: 0.1732\n",
      "Epoch: 021/050 | Batch 0200/0352 | Loss: 0.3187\n",
      "Epoch: 021/050 | Train Acc.: 86.247% |  Loss: 0.404\n",
      "Epoch: 021/050 | Validation Acc.: 70.800% |  Loss: 1.042\n",
      "Time elapsed: 26.28 min\n",
      "Epoch: 022/050 | Batch 0000/0352 | Loss: 0.1649\n",
      "Epoch: 022/050 | Batch 0200/0352 | Loss: 0.2494\n",
      "Epoch: 022/050 | Train Acc.: 86.980% |  Loss: 0.396\n",
      "Epoch: 022/050 | Validation Acc.: 70.400% |  Loss: 1.151\n",
      "Time elapsed: 27.55 min\n",
      "Epoch: 023/050 | Batch 0000/0352 | Loss: 0.0972\n",
      "Epoch: 023/050 | Batch 0200/0352 | Loss: 0.1658\n",
      "Epoch: 023/050 | Train Acc.: 92.431% |  Loss: 0.221\n",
      "Epoch: 023/050 | Validation Acc.: 74.240% |  Loss: 1.071\n",
      "Time elapsed: 28.82 min\n",
      "Epoch: 024/050 | Batch 0000/0352 | Loss: 0.1063\n",
      "Epoch: 024/050 | Batch 0200/0352 | Loss: 0.1249\n",
      "Epoch: 024/050 | Train Acc.: 90.587% |  Loss: 0.278\n",
      "Epoch: 024/050 | Validation Acc.: 73.500% |  Loss: 1.074\n",
      "Time elapsed: 30.09 min\n",
      "Epoch: 025/050 | Batch 0000/0352 | Loss: 0.1080\n",
      "Epoch: 025/050 | Batch 0200/0352 | Loss: 0.2313\n",
      "Epoch: 025/050 | Train Acc.: 95.371% |  Loss: 0.136\n",
      "Epoch: 025/050 | Validation Acc.: 75.840% |  Loss: 0.976\n",
      "Time elapsed: 31.36 min\n",
      "Epoch: 026/050 | Batch 0000/0352 | Loss: 0.0664\n",
      "Epoch: 026/050 | Batch 0200/0352 | Loss: 0.1232\n",
      "Epoch: 026/050 | Train Acc.: 93.111% |  Loss: 0.202\n",
      "Epoch: 026/050 | Validation Acc.: 73.820% |  Loss: 1.035\n",
      "Time elapsed: 32.63 min\n",
      "Epoch: 027/050 | Batch 0000/0352 | Loss: 0.0833\n",
      "Epoch: 027/050 | Batch 0200/0352 | Loss: 0.8488\n",
      "Epoch: 027/050 | Train Acc.: 92.671% |  Loss: 0.221\n",
      "Epoch: 027/050 | Validation Acc.: 72.800% |  Loss: 1.084\n",
      "Time elapsed: 33.90 min\n",
      "Epoch: 028/050 | Batch 0000/0352 | Loss: 0.0765\n",
      "Epoch: 028/050 | Batch 0200/0352 | Loss: 1.9833\n",
      "Epoch: 028/050 | Train Acc.: 34.849% |  Loss: 1.807\n",
      "Epoch: 028/050 | Validation Acc.: 34.300% |  Loss: 1.807\n",
      "Time elapsed: 35.17 min\n",
      "Epoch: 029/050 | Batch 0000/0352 | Loss: 1.6797\n",
      "Epoch: 029/050 | Batch 0200/0352 | Loss: 1.3071\n",
      "Epoch: 029/050 | Train Acc.: 58.767% |  Loss: 1.156\n",
      "Epoch: 029/050 | Validation Acc.: 56.420% |  Loss: 1.208\n",
      "Time elapsed: 36.44 min\n",
      "Epoch: 030/050 | Batch 0000/0352 | Loss: 1.1558\n",
      "Epoch: 030/050 | Batch 0200/0352 | Loss: 0.9122\n",
      "Epoch: 030/050 | Train Acc.: 67.989% |  Loss: 1.002\n",
      "Epoch: 030/050 | Validation Acc.: 62.540% |  Loss: 1.202\n",
      "Time elapsed: 37.72 min\n",
      "Epoch: 031/050 | Batch 0000/0352 | Loss: 0.6512\n",
      "Epoch: 031/050 | Batch 0200/0352 | Loss: 0.7850\n",
      "Epoch: 031/050 | Train Acc.: 74.280% |  Loss: 0.744\n",
      "Epoch: 031/050 | Validation Acc.: 67.900% |  Loss: 0.959\n",
      "Time elapsed: 38.99 min\n",
      "Epoch: 032/050 | Batch 0000/0352 | Loss: 0.6841\n",
      "Epoch: 032/050 | Batch 0200/0352 | Loss: 0.4345\n",
      "Epoch: 032/050 | Train Acc.: 84.002% |  Loss: 0.467\n",
      "Epoch: 032/050 | Validation Acc.: 73.080% |  Loss: 0.849\n",
      "Time elapsed: 40.26 min\n",
      "Epoch: 033/050 | Batch 0000/0352 | Loss: 0.3309\n",
      "Epoch: 033/050 | Batch 0200/0352 | Loss: 0.5741\n",
      "Epoch: 033/050 | Train Acc.: 92.038% |  Loss: 0.246\n",
      "Epoch: 033/050 | Validation Acc.: 74.300% |  Loss: 0.858\n",
      "Time elapsed: 41.53 min\n",
      "Epoch: 034/050 | Batch 0000/0352 | Loss: 0.1882\n",
      "Epoch: 034/050 | Batch 0200/0352 | Loss: 0.2683\n",
      "Epoch: 034/050 | Train Acc.: 95.740% |  Loss: 0.133\n",
      "Epoch: 034/050 | Validation Acc.: 74.560% |  Loss: 0.914\n",
      "Time elapsed: 42.80 min\n",
      "Epoch: 035/050 | Batch 0000/0352 | Loss: 0.0845\n",
      "Epoch: 035/050 | Batch 0200/0352 | Loss: 0.2656\n",
      "Epoch: 035/050 | Train Acc.: 96.229% |  Loss: 0.111\n",
      "Epoch: 035/050 | Validation Acc.: 75.500% |  Loss: 1.029\n",
      "Time elapsed: 44.07 min\n",
      "Epoch: 036/050 | Batch 0000/0352 | Loss: 0.0564\n",
      "Epoch: 036/050 | Batch 0200/0352 | Loss: 0.3159\n",
      "Epoch: 036/050 | Train Acc.: 91.829% |  Loss: 0.245\n",
      "Epoch: 036/050 | Validation Acc.: 71.960% |  Loss: 1.140\n",
      "Time elapsed: 45.34 min\n",
      "Epoch: 037/050 | Batch 0000/0352 | Loss: 0.0512\n",
      "Epoch: 037/050 | Batch 0200/0352 | Loss: 0.1552\n",
      "Epoch: 037/050 | Train Acc.: 96.244% |  Loss: 0.111\n",
      "Epoch: 037/050 | Validation Acc.: 74.520% |  Loss: 1.138\n",
      "Time elapsed: 46.61 min\n",
      "Epoch: 038/050 | Batch 0000/0352 | Loss: 0.0793\n",
      "Epoch: 038/050 | Batch 0200/0352 | Loss: 0.0956\n",
      "Epoch: 038/050 | Train Acc.: 96.993% |  Loss: 0.087\n",
      "Epoch: 038/050 | Validation Acc.: 74.280% |  Loss: 1.201\n",
      "Time elapsed: 47.88 min\n",
      "Epoch: 039/050 | Batch 0000/0352 | Loss: 0.1727\n",
      "Epoch: 039/050 | Batch 0200/0352 | Loss: 0.0446\n",
      "Epoch: 039/050 | Train Acc.: 96.813% |  Loss: 0.087\n",
      "Epoch: 039/050 | Validation Acc.: 74.920% |  Loss: 1.200\n",
      "Time elapsed: 49.15 min\n",
      "Epoch: 040/050 | Batch 0000/0352 | Loss: 0.0407\n",
      "Epoch: 040/050 | Batch 0200/0352 | Loss: 0.1799\n",
      "Epoch: 040/050 | Train Acc.: 96.267% |  Loss: 0.110\n",
      "Epoch: 040/050 | Validation Acc.: 75.420% |  Loss: 1.199\n",
      "Time elapsed: 50.41 min\n",
      "Epoch: 041/050 | Batch 0000/0352 | Loss: 0.1038\n",
      "Epoch: 041/050 | Batch 0200/0352 | Loss: 0.1297\n",
      "Epoch: 041/050 | Train Acc.: 91.278% |  Loss: 0.285\n",
      "Epoch: 041/050 | Validation Acc.: 70.780% |  Loss: 1.398\n",
      "Time elapsed: 51.68 min\n",
      "Epoch: 042/050 | Batch 0000/0352 | Loss: 0.0917\n",
      "Epoch: 042/050 | Batch 0200/0352 | Loss: 0.0908\n",
      "Epoch: 042/050 | Train Acc.: 94.051% |  Loss: 0.178\n",
      "Epoch: 042/050 | Validation Acc.: 73.500% |  Loss: 1.250\n",
      "Time elapsed: 52.95 min\n",
      "Epoch: 043/050 | Batch 0000/0352 | Loss: 0.0520\n",
      "Epoch: 043/050 | Batch 0200/0352 | Loss: 0.0387\n",
      "Epoch: 043/050 | Train Acc.: 97.891% |  Loss: 0.064\n",
      "Epoch: 043/050 | Validation Acc.: 75.980% |  Loss: 1.133\n",
      "Time elapsed: 54.22 min\n",
      "Epoch: 044/050 | Batch 0000/0352 | Loss: 0.0474\n",
      "Epoch: 044/050 | Batch 0200/0352 | Loss: 0.7054\n",
      "Epoch: 044/050 | Train Acc.: 90.498% |  Loss: 0.284\n",
      "Epoch: 044/050 | Validation Acc.: 72.200% |  Loss: 1.121\n",
      "Time elapsed: 55.49 min\n",
      "Epoch: 045/050 | Batch 0000/0352 | Loss: 0.1447\n",
      "Epoch: 045/050 | Batch 0200/0352 | Loss: 0.1145\n",
      "Epoch: 045/050 | Train Acc.: 97.609% |  Loss: 0.083\n",
      "Epoch: 045/050 | Validation Acc.: 75.100% |  Loss: 1.087\n",
      "Time elapsed: 56.76 min\n",
      "Epoch: 046/050 | Batch 0000/0352 | Loss: 0.0528\n",
      "Epoch: 046/050 | Batch 0200/0352 | Loss: 0.1134\n",
      "Epoch: 046/050 | Train Acc.: 98.529% |  Loss: 0.044\n",
      "Epoch: 046/050 | Validation Acc.: 74.360% |  Loss: 1.284\n",
      "Time elapsed: 58.03 min\n",
      "Epoch: 047/050 | Batch 0000/0352 | Loss: 0.0339\n",
      "Epoch: 047/050 | Batch 0200/0352 | Loss: 0.0187\n",
      "Epoch: 047/050 | Train Acc.: 96.691% |  Loss: 0.104\n",
      "Epoch: 047/050 | Validation Acc.: 74.080% |  Loss: 1.358\n",
      "Time elapsed: 59.30 min\n",
      "Epoch: 048/050 | Batch 0000/0352 | Loss: 0.0115\n",
      "Epoch: 048/050 | Batch 0200/0352 | Loss: 0.0237\n",
      "Epoch: 048/050 | Train Acc.: 98.431% |  Loss: 0.046\n",
      "Epoch: 048/050 | Validation Acc.: 74.880% |  Loss: 1.301\n",
      "Time elapsed: 60.58 min\n",
      "Epoch: 049/050 | Batch 0000/0352 | Loss: 0.0494\n",
      "Epoch: 049/050 | Batch 0200/0352 | Loss: 0.1029\n",
      "Epoch: 049/050 | Train Acc.: 97.551% |  Loss: 0.073\n",
      "Epoch: 049/050 | Validation Acc.: 75.200% |  Loss: 1.363\n",
      "Time elapsed: 61.85 min\n",
      "Epoch: 050/050 | Batch 0000/0352 | Loss: 0.0371\n",
      "Epoch: 050/050 | Batch 0200/0352 | Loss: 0.0507\n",
      "Epoch: 050/050 | Train Acc.: 96.562% |  Loss: 0.103\n",
      "Epoch: 050/050 | Validation Acc.: 74.320% |  Loss: 1.335\n",
      "Time elapsed: 63.13 min\n",
      "Total Training Time: 63.13 min\n"
     ]
    }
   ],
   "source": [
    "train_classifier_simple_v1(num_epochs=num_epochs, model=model, \n",
    "                           optimizer=optimizer, device=DEVICE, \n",
    "                           train_loader=train_loader, valid_loader=valid_loader, \n",
    "                           logging_interval=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, the deterministic behavior does not seem to influence performance noticeably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
