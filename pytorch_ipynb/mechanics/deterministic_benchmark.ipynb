{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Models -- A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.\n",
    "- Author: Sebastian Raschka\n",
    "- GitHub Repository: https://github.com/rasbt/deeplearning-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "\n",
      "CPython 3.7.3\n",
      "IPython 7.9.0\n",
      "\n",
      "torch 1.7.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Runs on CPU or GPU (if available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo -- Reproducible Results with Deterministic Behavior and Runtime Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are benchmarking the performance impact of setting PyTorch to deterministic behavior. In general, there are two aspects for reproducible resuls in PyTorch, \n",
    "1. Setting a random seed\n",
    "2. Setting cuDNN and PyTorch algorithmic behavior to deterministic\n",
    "\n",
    "For more details, please see https://pytorch.org/docs/stable/notes/randomness.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting a random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend using a function like the following one prior to using dataset loaders and initializing a model if you want to ensure the data is shuffled in the same manner if you rerun this notebook and the model gets the same initial random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting cuDNN and PyTorch algorithmic behavior to deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the `set_all_seeds` function above, I recommend setting the behavior of PyTorch and cuDNN to deterministic (this is particulary relevant when using GPUs). We can also define a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_deterministic():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    torch.set_deterministic(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the general configuration in this section, the following two sections will train a ResNet-101 model without and with deterministic behavior to get a sense how using deterministic options affect the runtime speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Device\n",
    "CUDA_DEVICE_NUM = 1 # change as appropriate\n",
    "DEVICE = torch.device('cuda:%d' % CUDA_DEVICE_NUM if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# Data settings\n",
    "num_classes = 10\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\") # to include ../helper_evaluate.py etc.\n",
    "\n",
    "from helper_evaluate import compute_accuracy\n",
    "from helper_data import get_dataloaders_cifar10\n",
    "from helper_train import train_classifier_simple_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Run without Deterministic Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we enable deterministic behavior, we will run a ResNet-101 with otherwise the exact same settings for comparison. Note that setting random seeds doesn't affect the timing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set random seed ###\n",
    "set_all_seeds(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### Dataset\n",
    "##########################\n",
    "\n",
    "train_loader, valid_loader, test_loader = get_dataloaders_cifar10(\n",
    "    batch_size, \n",
    "    num_workers=0, \n",
    "    validation_fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### Model\n",
    "##########################\n",
    "\n",
    "\n",
    "from deterministic_benchmark_utils import resnet101\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = resnet101(num_classes, grayscale=False)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Batch 0000/0352 | Loss: 2.6271\n",
      "Epoch: 001/050 | Batch 0200/0352 | Loss: 2.2099\n",
      "***Epoch: 001/050 | Train. Acc.: 14.251% |  Loss: 2.428\n",
      "***Epoch: 001/050 | Valid. Acc.: 13.980% |  Loss: 2.411\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 002/050 | Batch 0000/0352 | Loss: 1.9708\n",
      "Epoch: 002/050 | Batch 0200/0352 | Loss: 1.7608\n",
      "***Epoch: 002/050 | Train. Acc.: 39.780% |  Loss: 1.603\n",
      "***Epoch: 002/050 | Valid. Acc.: 39.220% |  Loss: 1.583\n",
      "Time elapsed: 2.39 min\n",
      "Epoch: 003/050 | Batch 0000/0352 | Loss: 1.5973\n",
      "Epoch: 003/050 | Batch 0200/0352 | Loss: 1.4077\n",
      "***Epoch: 003/050 | Train. Acc.: 46.629% |  Loss: 1.451\n",
      "***Epoch: 003/050 | Valid. Acc.: 45.420% |  Loss: 1.450\n",
      "Time elapsed: 3.65 min\n",
      "Epoch: 004/050 | Batch 0000/0352 | Loss: 1.4408\n",
      "Epoch: 004/050 | Batch 0200/0352 | Loss: 1.3188\n",
      "***Epoch: 004/050 | Train. Acc.: 51.842% |  Loss: 1.325\n",
      "***Epoch: 004/050 | Valid. Acc.: 49.760% |  Loss: 1.369\n",
      "Time elapsed: 4.88 min\n",
      "Epoch: 005/050 | Batch 0000/0352 | Loss: 1.2832\n",
      "Epoch: 005/050 | Batch 0200/0352 | Loss: 1.2371\n",
      "***Epoch: 005/050 | Train. Acc.: 47.622% |  Loss: 1.473\n",
      "***Epoch: 005/050 | Valid. Acc.: 46.560% |  Loss: 1.504\n",
      "Time elapsed: 6.11 min\n",
      "Epoch: 006/050 | Batch 0000/0352 | Loss: 1.1231\n",
      "Epoch: 006/050 | Batch 0200/0352 | Loss: 1.1939\n",
      "***Epoch: 006/050 | Train. Acc.: 55.778% |  Loss: 1.265\n",
      "***Epoch: 006/050 | Valid. Acc.: 53.400% |  Loss: 1.315\n",
      "Time elapsed: 7.34 min\n",
      "Epoch: 007/050 | Batch 0000/0352 | Loss: 1.0269\n",
      "Epoch: 007/050 | Batch 0200/0352 | Loss: 1.1000\n",
      "***Epoch: 007/050 | Train. Acc.: 38.996% |  Loss: 7.603\n",
      "***Epoch: 007/050 | Valid. Acc.: 38.860% |  Loss: 7.870\n",
      "Time elapsed: 8.56 min\n",
      "Epoch: 008/050 | Batch 0000/0352 | Loss: 1.2041\n",
      "Epoch: 008/050 | Batch 0200/0352 | Loss: 0.9986\n",
      "***Epoch: 008/050 | Train. Acc.: 63.844% |  Loss: 1.041\n",
      "***Epoch: 008/050 | Valid. Acc.: 60.980% |  Loss: 1.136\n",
      "Time elapsed: 9.78 min\n",
      "Epoch: 009/050 | Batch 0000/0352 | Loss: 0.8854\n",
      "Epoch: 009/050 | Batch 0200/0352 | Loss: 0.9018\n",
      "***Epoch: 009/050 | Train. Acc.: 70.673% |  Loss: 0.859\n",
      "***Epoch: 009/050 | Valid. Acc.: 66.160% |  Loss: 1.011\n",
      "Time elapsed: 10.99 min\n",
      "Epoch: 010/050 | Batch 0000/0352 | Loss: 0.7884\n",
      "Epoch: 010/050 | Batch 0200/0352 | Loss: 0.9884\n",
      "***Epoch: 010/050 | Train. Acc.: 67.833% |  Loss: 2.588\n",
      "***Epoch: 010/050 | Valid. Acc.: 62.060% |  Loss: 2.727\n",
      "Time elapsed: 12.22 min\n",
      "Epoch: 011/050 | Batch 0000/0352 | Loss: 0.7723\n",
      "Epoch: 011/050 | Batch 0200/0352 | Loss: 0.9763\n",
      "***Epoch: 011/050 | Train. Acc.: 10.753% |  Loss: 38.823\n",
      "***Epoch: 011/050 | Valid. Acc.: 10.520% |  Loss: 40.776\n",
      "Time elapsed: 13.44 min\n",
      "Epoch: 012/050 | Batch 0000/0352 | Loss: 1.0580\n",
      "Epoch: 012/050 | Batch 0200/0352 | Loss: 1.6218\n",
      "***Epoch: 012/050 | Train. Acc.: 50.164% |  Loss: 1.440\n",
      "***Epoch: 012/050 | Valid. Acc.: 49.920% |  Loss: 1.446\n",
      "Time elapsed: 14.67 min\n",
      "Epoch: 013/050 | Batch 0000/0352 | Loss: 1.3548\n",
      "Epoch: 013/050 | Batch 0200/0352 | Loss: 1.0921\n",
      "***Epoch: 013/050 | Train. Acc.: 66.160% |  Loss: 1.063\n",
      "***Epoch: 013/050 | Valid. Acc.: 63.300% |  Loss: 1.143\n",
      "Time elapsed: 15.90 min\n",
      "Epoch: 014/050 | Batch 0000/0352 | Loss: 0.9895\n",
      "Epoch: 014/050 | Batch 0200/0352 | Loss: 0.9918\n",
      "***Epoch: 014/050 | Train. Acc.: 70.311% |  Loss: 0.853\n",
      "***Epoch: 014/050 | Valid. Acc.: 66.520% |  Loss: 0.967\n",
      "Time elapsed: 17.13 min\n",
      "Epoch: 015/050 | Batch 0000/0352 | Loss: 0.7741\n",
      "Epoch: 015/050 | Batch 0200/0352 | Loss: 0.9537\n",
      "***Epoch: 015/050 | Train. Acc.: 77.804% |  Loss: 0.663\n",
      "***Epoch: 015/050 | Valid. Acc.: 72.120% |  Loss: 0.835\n",
      "Time elapsed: 18.36 min\n",
      "Epoch: 016/050 | Batch 0000/0352 | Loss: 0.5985\n",
      "Epoch: 016/050 | Batch 0200/0352 | Loss: 0.8123\n",
      "***Epoch: 016/050 | Train. Acc.: 73.247% |  Loss: 0.803\n",
      "***Epoch: 016/050 | Valid. Acc.: 65.780% |  Loss: 1.037\n",
      "Time elapsed: 19.61 min\n",
      "Epoch: 017/050 | Batch 0000/0352 | Loss: 0.5573\n",
      "Epoch: 017/050 | Batch 0200/0352 | Loss: 0.6740\n",
      "***Epoch: 017/050 | Train. Acc.: 71.811% |  Loss: 0.828\n",
      "***Epoch: 017/050 | Valid. Acc.: 65.740% |  Loss: 1.038\n",
      "Time elapsed: 20.85 min\n",
      "Epoch: 018/050 | Batch 0000/0352 | Loss: 0.5445\n",
      "Epoch: 018/050 | Batch 0200/0352 | Loss: 0.5777\n",
      "***Epoch: 018/050 | Train. Acc.: 77.591% |  Loss: 0.658\n",
      "***Epoch: 018/050 | Valid. Acc.: 68.560% |  Loss: 1.012\n",
      "Time elapsed: 22.07 min\n",
      "Epoch: 019/050 | Batch 0000/0352 | Loss: 0.3933\n",
      "Epoch: 019/050 | Batch 0200/0352 | Loss: 0.4826\n",
      "***Epoch: 019/050 | Train. Acc.: 74.096% |  Loss: 0.771\n",
      "***Epoch: 019/050 | Valid. Acc.: 65.080% |  Loss: 1.150\n",
      "Time elapsed: 23.29 min\n",
      "Epoch: 020/050 | Batch 0000/0352 | Loss: 0.3610\n",
      "Epoch: 020/050 | Batch 0200/0352 | Loss: 0.5321\n",
      "***Epoch: 020/050 | Train. Acc.: 81.602% |  Loss: 0.560\n",
      "***Epoch: 020/050 | Valid. Acc.: 69.800% |  Loss: 1.086\n",
      "Time elapsed: 24.51 min\n",
      "Epoch: 021/050 | Batch 0000/0352 | Loss: 0.2634\n",
      "Epoch: 021/050 | Batch 0200/0352 | Loss: 0.4697\n",
      "***Epoch: 021/050 | Train. Acc.: 76.991% |  Loss: 0.721\n",
      "***Epoch: 021/050 | Valid. Acc.: 65.720% |  Loss: 1.264\n",
      "Time elapsed: 25.76 min\n",
      "Epoch: 022/050 | Batch 0000/0352 | Loss: 0.3080\n",
      "Epoch: 022/050 | Batch 0200/0352 | Loss: 0.3277\n",
      "***Epoch: 022/050 | Train. Acc.: 84.564% |  Loss: 0.469\n",
      "***Epoch: 022/050 | Valid. Acc.: 71.000% |  Loss: 1.111\n",
      "Time elapsed: 27.00 min\n",
      "Epoch: 023/050 | Batch 0000/0352 | Loss: 0.1788\n",
      "Epoch: 023/050 | Batch 0200/0352 | Loss: 0.1991\n",
      "***Epoch: 023/050 | Train. Acc.: 80.418% |  Loss: 0.631\n",
      "***Epoch: 023/050 | Valid. Acc.: 66.860% |  Loss: 1.297\n",
      "Time elapsed: 28.23 min\n",
      "Epoch: 024/050 | Batch 0000/0352 | Loss: 0.2006\n",
      "Epoch: 024/050 | Batch 0200/0352 | Loss: 0.3024\n",
      "***Epoch: 024/050 | Train. Acc.: 84.909% |  Loss: 0.460\n",
      "***Epoch: 024/050 | Valid. Acc.: 69.540% |  Loss: 1.220\n",
      "Time elapsed: 29.46 min\n",
      "Epoch: 025/050 | Batch 0000/0352 | Loss: 0.1478\n",
      "Epoch: 025/050 | Batch 0200/0352 | Loss: 0.2084\n",
      "***Epoch: 025/050 | Train. Acc.: 84.747% |  Loss: 0.486\n",
      "***Epoch: 025/050 | Valid. Acc.: 68.620% |  Loss: 1.281\n",
      "Time elapsed: 30.69 min\n",
      "Epoch: 026/050 | Batch 0000/0352 | Loss: 0.1907\n",
      "Epoch: 026/050 | Batch 0200/0352 | Loss: 0.2614\n",
      "***Epoch: 026/050 | Train. Acc.: 88.716% |  Loss: 0.354\n",
      "***Epoch: 026/050 | Valid. Acc.: 71.480% |  Loss: 1.234\n",
      "Time elapsed: 31.93 min\n",
      "Epoch: 027/050 | Batch 0000/0352 | Loss: 0.1498\n",
      "Epoch: 027/050 | Batch 0200/0352 | Loss: 0.1500\n",
      "***Epoch: 027/050 | Train. Acc.: 85.364% |  Loss: 0.476\n",
      "***Epoch: 027/050 | Valid. Acc.: 69.480% |  Loss: 1.361\n",
      "Time elapsed: 33.16 min\n",
      "Epoch: 028/050 | Batch 0000/0352 | Loss: 0.2031\n",
      "Epoch: 028/050 | Batch 0200/0352 | Loss: 0.2648\n",
      "***Epoch: 028/050 | Train. Acc.: 59.271% |  Loss: 2.780\n",
      "***Epoch: 028/050 | Valid. Acc.: 53.360% |  Loss: 2.929\n",
      "Time elapsed: 34.40 min\n",
      "Epoch: 029/050 | Batch 0000/0352 | Loss: 0.5912\n",
      "Epoch: 029/050 | Batch 0200/0352 | Loss: 0.8881\n",
      "***Epoch: 029/050 | Train. Acc.: 56.573% |  Loss: 1.407\n",
      "***Epoch: 029/050 | Valid. Acc.: 53.540% |  Loss: 1.531\n",
      "Time elapsed: 35.64 min\n",
      "Epoch: 030/050 | Batch 0000/0352 | Loss: 1.1182\n",
      "Epoch: 030/050 | Batch 0200/0352 | Loss: 0.6337\n",
      "***Epoch: 030/050 | Train. Acc.: 88.293% |  Loss: 0.351\n",
      "***Epoch: 030/050 | Valid. Acc.: 73.080% |  Loss: 0.874\n",
      "Time elapsed: 36.87 min\n",
      "Epoch: 031/050 | Batch 0000/0352 | Loss: 0.2898\n",
      "Epoch: 031/050 | Batch 0200/0352 | Loss: 0.3161\n",
      "***Epoch: 031/050 | Train. Acc.: 90.947% |  Loss: 0.269\n",
      "***Epoch: 031/050 | Valid. Acc.: 72.240% |  Loss: 1.147\n",
      "Time elapsed: 38.10 min\n",
      "Epoch: 032/050 | Batch 0000/0352 | Loss: 0.1035\n",
      "Epoch: 032/050 | Batch 0200/0352 | Loss: 0.1304\n",
      "***Epoch: 032/050 | Train. Acc.: 90.316% |  Loss: 0.297\n",
      "***Epoch: 032/050 | Valid. Acc.: 72.240% |  Loss: 1.282\n",
      "Time elapsed: 39.33 min\n",
      "Epoch: 033/050 | Batch 0000/0352 | Loss: 0.1485\n",
      "Epoch: 033/050 | Batch 0200/0352 | Loss: 0.2556\n",
      "***Epoch: 033/050 | Train. Acc.: 93.631% |  Loss: 0.195\n",
      "***Epoch: 033/050 | Valid. Acc.: 73.460% |  Loss: 1.269\n",
      "Time elapsed: 40.57 min\n",
      "Epoch: 034/050 | Batch 0000/0352 | Loss: 0.0457\n",
      "Epoch: 034/050 | Batch 0200/0352 | Loss: 0.1334\n",
      "***Epoch: 034/050 | Train. Acc.: 93.564% |  Loss: 0.196\n",
      "***Epoch: 034/050 | Valid. Acc.: 73.360% |  Loss: 1.267\n",
      "Time elapsed: 41.80 min\n",
      "Epoch: 035/050 | Batch 0000/0352 | Loss: 0.0680\n",
      "Epoch: 035/050 | Batch 0200/0352 | Loss: 0.0651\n",
      "***Epoch: 035/050 | Train. Acc.: 93.942% |  Loss: 0.186\n",
      "***Epoch: 035/050 | Valid. Acc.: 74.080% |  Loss: 1.315\n",
      "Time elapsed: 43.05 min\n",
      "Epoch: 036/050 | Batch 0000/0352 | Loss: 0.1039\n",
      "Epoch: 036/050 | Batch 0200/0352 | Loss: 0.0776\n",
      "***Epoch: 036/050 | Train. Acc.: 95.662% |  Loss: 0.129\n",
      "***Epoch: 036/050 | Valid. Acc.: 74.680% |  Loss: 1.279\n",
      "Time elapsed: 44.28 min\n",
      "Epoch: 037/050 | Batch 0000/0352 | Loss: 0.0636\n",
      "Epoch: 037/050 | Batch 0200/0352 | Loss: 0.0624\n",
      "***Epoch: 037/050 | Train. Acc.: 95.158% |  Loss: 0.149\n",
      "***Epoch: 037/050 | Valid. Acc.: 74.660% |  Loss: 1.282\n",
      "Time elapsed: 45.52 min\n",
      "Epoch: 038/050 | Batch 0000/0352 | Loss: 0.1087\n",
      "Epoch: 038/050 | Batch 0200/0352 | Loss: 0.1315\n",
      "***Epoch: 038/050 | Train. Acc.: 93.062% |  Loss: 0.223\n",
      "***Epoch: 038/050 | Valid. Acc.: 71.700% |  Loss: 1.470\n",
      "Time elapsed: 46.76 min\n",
      "Epoch: 039/050 | Batch 0000/0352 | Loss: 0.0721\n",
      "Epoch: 039/050 | Batch 0200/0352 | Loss: 0.0955\n",
      "***Epoch: 039/050 | Train. Acc.: 93.427% |  Loss: 0.213\n",
      "***Epoch: 039/050 | Valid. Acc.: 73.740% |  Loss: 1.394\n",
      "Time elapsed: 48.03 min\n",
      "Epoch: 040/050 | Batch 0000/0352 | Loss: 0.0796\n",
      "Epoch: 040/050 | Batch 0200/0352 | Loss: 0.1094\n",
      "***Epoch: 040/050 | Train. Acc.: 94.271% |  Loss: 0.189\n",
      "***Epoch: 040/050 | Valid. Acc.: 73.400% |  Loss: 1.413\n",
      "Time elapsed: 49.26 min\n",
      "Epoch: 041/050 | Batch 0000/0352 | Loss: 0.0644\n",
      "Epoch: 041/050 | Batch 0200/0352 | Loss: 0.0317\n",
      "***Epoch: 041/050 | Train. Acc.: 91.882% |  Loss: 0.295\n",
      "***Epoch: 041/050 | Valid. Acc.: 72.520% |  Loss: 1.624\n",
      "Time elapsed: 50.49 min\n",
      "Epoch: 042/050 | Batch 0000/0352 | Loss: 0.0423\n",
      "Epoch: 042/050 | Batch 0200/0352 | Loss: 0.0265\n",
      "***Epoch: 042/050 | Train. Acc.: 88.069% |  Loss: 0.497\n",
      "***Epoch: 042/050 | Valid. Acc.: 69.260% |  Loss: 1.725\n",
      "Time elapsed: 51.71 min\n",
      "Epoch: 043/050 | Batch 0000/0352 | Loss: 0.0699\n",
      "Epoch: 043/050 | Batch 0200/0352 | Loss: 0.1983\n",
      "***Epoch: 043/050 | Train. Acc.: 96.851% |  Loss: 0.096\n",
      "***Epoch: 043/050 | Valid. Acc.: 75.040% |  Loss: 1.062\n",
      "Time elapsed: 52.93 min\n",
      "Epoch: 044/050 | Batch 0000/0352 | Loss: 0.0765\n",
      "Epoch: 044/050 | Batch 0200/0352 | Loss: 0.0261\n",
      "***Epoch: 044/050 | Train. Acc.: 97.400% |  Loss: 0.077\n",
      "***Epoch: 044/050 | Valid. Acc.: 75.580% |  Loss: 1.322\n",
      "Time elapsed: 54.16 min\n",
      "Epoch: 045/050 | Batch 0000/0352 | Loss: 0.0228\n"
     ]
    }
   ],
   "source": [
    "train_classifier_simple_v1(num_epochs=num_epochs, model=model, \n",
    "                           optimizer=optimizer, device=DEVICE, \n",
    "                           train_loader=train_loader, valid_loader=valid_loader, \n",
    "                           logging_interval=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Run with Deterministic Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we set the deterministic behavior via the `set_deterministic()` function defined at the top of this notebook and compare how it affects the runtime speed of the ResNet-101 model. (Note that setting random seeds doesn't affect the timing results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set random seed ###\n",
    "set_all_seeds(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### Dataset\n",
    "##########################\n",
    "\n",
    "train_loader, valid_loader, test_loader = get_dataloaders_cifar10(\n",
    "    batch_size, \n",
    "    num_workers=0, \n",
    "    validation_fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### Model\n",
    "##########################\n",
    "\n",
    "\n",
    "from deterministic_benchmark_utils import resnet101\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = resnet101(num_classes, grayscale=False)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classifier_simple_v1(num_epochs=num_epochs, model=model, \n",
    "                           optimizer=optimizer, device=DEVICE, \n",
    "                           train_loader=train_loader, valid_loader=valid_loader, \n",
    "                           logging_interval=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, the deterministic behavior does not seem to influence performance noticeably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
